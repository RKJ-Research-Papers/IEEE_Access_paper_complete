\section{Introduction}

Credit-risk assessment plays a central role in financial decision-making, shaping lending policies and capital allocation in regulated institutions \parencite{Baesens2003_benchmarking,Lessmann2015_benchmarking}. Contemporary machine-learning models have substantially improved predictive discrimination, yet their explanations remain epistemically untested. A widespread assumption persists in practice: strong predictive performance implies reliable model explanations. This assumption is false. A model's ability to discriminate borrowers by default risk does not guarantee that its feature attributions reflect genuine learned structure rather than statistical artefacts, sampling noise, or spurious correlations \parencite{Hassija2024_interpreting}. This disconnect between prediction and explanation constitutes a fundamental scientific problem: explanations are treated as credible outputs rather than as empirical hypotheses subject to rigorous validation.

The challenge is compounded by the proliferation of post-hoc explainability tools and generative AI. SHAP and LIME assign numerical attributions to features, addressing some interpretability concerns in high-accuracy ensemble models \parencite{Ribeiro2016_why,Lundberg2017_unified}. However, growing evidence indicates that these attribution methods are highly sensitive to background distributions and sampling noise, frequently reflecting artefacts rather than genuine model structure \parencite{Slack2020_fooling,Hassija2024_interpreting}. Recent applications of generative AI that translate attribution scores into natural-language explanations remain largely unconstrained, further amplifying risks of hallucination, narrative inflation, and spurious causal claims \parencite{Hassija2024_interpreting}. The result is a proliferation of confident-sounding explanations with no mechanism to assess their empirical grounding.

This fragmentation exposes a deeper methodological gap: explainability in credit-risk modelling is rarely treated as a falsifiable scientific process. Existing work predominantly frames explanations as descriptive summaries or visualisation artefacts rather than as testable hypotheses subject to systematic evaluation. No prior framework integrates predictive modelling, attribution stability assessment, and constrained generative reasoning while explicitly quantifying whether explanatory signals persist under empirical scrutiny \parencite{Slack2020_fooling,Hassija2024_interpreting}.

This paper addresses this gap by proposing a falsifiability framework that reframes explainability from narrative generation to empirical validation. The core insight is simple but consequential: treat explanations as claims subject to reliability testing. The framework introduces three epistemically distinct components: (1) dual-selector feature stabilisation that grounds attributions in multiple signal sources prior to model training; (2) reliability diagnostics based on sanity ratio validation and signal-to-noise discrimination, enabling explicit assessment of whether attribution signals are robust or spurious; and (3) a constrained generative-AI module that produces human-readable explanations only when reliability thresholds are met, accompanied by explicit uncertainty qualifications. Rather than suppressing explanations with low reliability, the framework routes them to the generative module with uncertainty caveats, ensuring stakeholders are never presented with unfounded confidence.

To stabilise the features used for explanation, the framework adopts the Feature Selector-classifier Optimization Framework proposed by Zeng et al. \parencite{Zeng2024_ensemble}. Their dual-selector mechanism—combining nonlinear Random Forest importance with sparse L1-regularised logistic regression coefficients—establishes a stable feature foundation prior to SHAP analysis. This hybrid design reduces estimator bias while preserving both interaction-aware and linear structural signals, creating the epistemic conditions necessary for trustworthy attribution.

Using the German Credit dataset as a controlled benchmark, the framework is evaluated across a broad family of calibrated classification models to assess both predictive performance and the empirical reliability of their explanations \parencite{Baesens2003_benchmarking,Lessmann2015_benchmarking}. The results reveal a structural paradox: explanation quality varies independently of predictive accuracy, and many high-performing models produce feature attributions flagged as unreliable by sanity checks. By providing explicit, quantifiable diagnostics for explanation reliability, this work establishes a foundation for treating explainability as a scientifically testable—and falsifiable—component of model validity rather than as a cosmetic add-on. The implications extend to regulatory compliance: financial institutions can now ground model governance in transparent, auditable explanations rather than in subjective narratives.
